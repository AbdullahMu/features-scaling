{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Feature Scaling\n",
    "\n",
    "_Authors: Kiefer Katovich (SF), Joseph Nelson (DC)_\n",
    "\n",
    "---\n",
    "\n",
    "### Learning Objectives\n",
    "- Explain the benefits of scaling data.\n",
    "- Identify situations in which scaling data is beneficial. \n",
    "- Scale data using Python and scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Lesson Guide\n",
    "- [Introduction to Feature Scaling](#intro)\n",
    "- [Why Scale Data?](#why-scale)\n",
    "- [Centering](#centering)\n",
    "- [Standardization](#standardization)\n",
    "    - [Standardizing with scikit-learn's `StandardScaler`](#standard-scaler)\n",
    "- [Normalization](#normalization)\n",
    "    - [Normalizing With scikit-learn's `MinMaxScaler`](#minmax)\n",
    "- [Independent Practice Scaling the Wine Data Set](#independent-practice)\n",
    "- [Additional Resources](#resources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "\n",
    "## Introduction to Feature Scaling\n",
    "\n",
    "---\n",
    "\n",
    "Scaling data is the process of increasing or decreasing its magnitude according to a fixed ratio. In other words, you change the size but not the shape of the data (the shape of the distribution remains unchanged).\n",
    "\n",
    "Some data scaling methods often change the *location* of the data as well. For example, when \"centering\" we take a distribution and change its mean to zero by subtracting the mean of the distribution from each data point in the distribution. While this is not technically \"scaling,\" changing the location is often part of the process and preserves the shape of the data set (it just shifts it around).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='why-scale'></a>\n",
    "\n",
    "## Why Should We Scale Data?\n",
    "\n",
    "---\n",
    "\n",
    "**There are a number of good reasons to scale our data:**\n",
    "- To handle disparities in units.\n",
    "- To cut computational expense.\n",
    "- To improve model performance (especially for machine learning).\n",
    "- We scale for models to prevent the steps on different axes from varying widely.\n",
    "\n",
    "**It’s rarely a bad idea to scale your data.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='centering'></a>\n",
    "\n",
    "## Centering: Changing the Location of Data\n",
    "\n",
    "---\n",
    "\n",
    "Let's start with the simplest transformation example — centering. If we have a distribution of values ($X$), then to center our data to a new distribution ($X_c$) we apply the following formula:\n",
    "\n",
    "### $$ X_c = X - \\bar{X} $$\n",
    "\n",
    "### Benefits of Centering Data\n",
    "\n",
    "In linear modeling, the primary benefit of centering your predictor data is that **the intercept now represents the estimate of the target when all predictors are at their mean value.**\n",
    "\n",
    "If we don't center, the intercept is the estimate of our model when all predictors are at value 0. When you center your predictors, it often makes the intercept much more interpretable.\n",
    "\n",
    "### Centering Example: Baseball Player Height and Weight\n",
    "\n",
    "Load in the data set containing the heights, weights, and ages of baseball players."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseball = pd.read_csv('./datasets/baseball_height_weight.csv')\n",
    "baseball.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot the distribution of the heights and weights below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Construct a linear regression predicting weight from height. Interpret the value of the intercept and the coefficient from this model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Center the height variable and rerun the regression with the centered height. Interpret the new intercept and coefficient.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do centering for height\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of centered heights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Construct a linear regression predicting weight from centered height. Interpret the value of the intercept and the coefficient from this model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean height\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='standardization'></a>\n",
    "\n",
    "## Standardization\n",
    "\n",
    "---\n",
    "\n",
    "The most common method of scaling is standardization. In standardization, we first center the data and then divide by the standard deviation to enforce that the standard deviation of the variable is one:\n",
    "\n",
    "### $$ X_{std} = \\frac{X - \\bar{X}}{s_{X}} $$\n",
    "\n",
    "### Standardization Example\n",
    "\n",
    "First, plot the original `height` variable against the `weight` variable. Use seaborn's `sns.jointplot`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create standardized versions of the height and weight variables.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseball['height_std'] = \n",
    "baseball['weight_std'] = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot the standardized weight against the height. Notice the distribution shapes and relationship between the variables is unchanged.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='standard-scaler'></a>\n",
    "### Standardizing with scikit-learn's `StandardScaler`\n",
    "\n",
    "Scikit-learn comes packaged with a class, `StandardScaler`, that will perform the standardization on a matrix for you. \n",
    "\n",
    "Load in the package like so:\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "```\n",
    "\n",
    "Once instantiated, the `StandardScaler` object has three primary built-in methods:\n",
    "- `.fit(X)` will calculate the mean and standard deviations for each column of X.\n",
    "- `.transform(X)` will take X and return a transformed version of X where each column is standardized according to their means and standard deviations (you must have run `.fit()` first).\n",
    "- `.fit_transform(X)` combines the `.fit()` method and the `.transform()` method.\n",
    "\n",
    "**Use `StandardScaler` to standardize a predictor matrix containing heights and weights from the baseball data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize the scaler.\n",
    "ss = \n",
    "\n",
    "# Fit the data using the scaler (scale the data).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build a linear regression predicting age from the standardized height and weight data. Interpret the coefficients.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='normalization'></a>\n",
    "\n",
    "## Normalization\n",
    "\n",
    "---\n",
    "\n",
    "Normalization most often refers to the process of \"normalizing\" a variable to exist between 0 and 1. Think of it as squishing the variable to restrict it to a specific range.\n",
    "\n",
    "### $$ X_{norm} = \\frac{X - min(X)}{max(X) - min(X)} $$\n",
    "\n",
    "This type of normalization is typically referred to as \"min-max scaling.\" \n",
    "\n",
    "### Benefits of Normalization\n",
    "\n",
    "Typically, standardization is preferred to min-max normalization. However, there are some applications where min-max scaling is preferable:\n",
    "- In neural networks, for example, which often require their inputs to be bounded between 0 and 1. \n",
    "- In images where pixels can only take on a specific range of RGB values.\n",
    "\n",
    "<a id='minmax'></a>\n",
    "### Normalization with `MinMaxScaler`\n",
    "\n",
    "Scikit-learn also has a class for normalization called `MinMaxScaler`:\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "```\n",
    "\n",
    "The `MinMaxScaler` has the same `fit()`, `transform()`, and `fit_transform()` methods.\n",
    "\n",
    "**Normalize the `height`, and `weight` variables using `MinMaxScaler.`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "# Initialize the scaler.\n",
    "mms = MinMaxScaler()\n",
    "\n",
    "# Fit the data using the scaler (scale the data).\n",
    "Xn = mms.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Look at the min and max ranges for the normalized matrix.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot the normalized `height` against the normalized `weight`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build a linear regression predicting age from the standardized height and weight data. Interpret the coefficients.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='independent-practice'></a>\n",
    "\n",
    "## Independent Practice: Scaling the Wine Data Set\n",
    "\n",
    "---\n",
    "\n",
    "Below you'll load in the wine quality data set. This data set contains a variety of features for different types/brands of wine. \n",
    "\n",
    "**You should:**\n",
    "1) Load and examine the data.\n",
    "2) Create a target variable for wine quality.\n",
    "3) Create a predictor matrix with the variables of your choice.\n",
    "4) Create standardized and normalized versions of your predictor matrix.\n",
    "5) Employing cross-validation, calculate the average $R^2$ score for wine quality using the original predictors, the standardized predictors, and the normalized predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1) Load and examine the data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = pd.read_csv('./datasets/winequality_merged.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2) Create a target variable for wine quality.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3) Create a predictor matrix with variables of your choice.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4) Create a standardized and normalized version of your predictor matrix.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5) Employing cross-validation, calculate the average $R^2$ score for wine quality using the original predictors, the standardized predictors, and the normalized predictors.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='resources'></a>\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "---\n",
    "\n",
    "[About Feature Scaling and Normalization](http://sebastianraschka.com/Articles/2014_about_feature_scaling.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
